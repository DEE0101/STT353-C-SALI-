---
title: "Chapter 7 Time series regression models"
author: "JUDISMA A. SALI"
date: "2023-07-24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 7 Time Series Regression Models

Forecasting the time series of interest y assuming that it has a linear relationship with other time series x.

```{r}
library(fpp3)
```


# 7.1.1 Simple linear regression

In the simplest case, the regression model allows for a linear relationship between the forecast variable $y$ and a single predictor variable $x$: $y_t = \beta_0 + \beta_1 x_1 + \epsilon_t$

Where the coefficients $\beta_0$ and $\beta_1$ denote the intercept and the slope of the line, respectively. $\epsilon_t$ is a white noise error term.

For illustration, use the US consumption data `us_change`, to fit a simple linear model where `Consumption` is predicted against `Income`. First, plot these two time series.

```{r}
us_change |>
  pivot_longer(c(Consumption, Income), names_to="Series") |>
  autoplot(value) +
  labs(y = "% change")
```

The plot shows time series of quarterly percentage changes (growth rates) of personal consumption expenditure and personal income for the US from 1970 Q1 to 2019 Q2.

And then make a scatter plot:

```{r}
us_change |>
  ggplot(aes(x = Income, y = Consumption)) +
  labs(y = "Consumption (quarterly % change)",
       x = "Income (quarterly % change)") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

The fitted line has a positive slope, reflecting the positive relationship between income and consumption. 

The equation is estimated using the TSLM() function:

```{r}
us_change |>
  model(TSLM(Consumption ~ Income)) |>
  report()
```

The simple linear model can be written as 

Consumption $= 0.54 + 0.27 (Income) + \epsilon$

The slope coefficient shows that a one unit increase in personal disposable income results on average in 0.27 units increase in personal consumption expenditure. When there is no change in personal disposable income since the last quarter, the average increase in personal consumption expenditure of 0.54%.

Therefore, there is a significant positive relationship between personal disposable income and personal consumption expenditure of US consumption data.

# 7.1.2 Multiple linear regression

When there are two or more predictor variables, the model is called a multiple regression model. The general form of a multiple regression model is

$y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + \dots + \beta_k x_{kt}$

where $y$ is the variable to be forecast and $x_1, \dots, x_k$ are the $k$ predictor variables. The coefficients $\beta_0, \beta_1,   \dots , \beta_k$ measure the effect of each predictor after taking into account the effects of all the other predictors in the model. 

We could simply use more predictors in `us_change` to create a multiple linear regression model. This time, `Production`, `Savings` and `Unemployment` are included in the model. 

```{r}
us_change |>
  select(-Consumption, -Income) |>
  pivot_longer(-Quarter) |>
  ggplot(aes(Quarter, value, colour = name)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  guides(colour = "none") +
  labs(y="% change")
```
The plots show the quarterly percentage changes in industrial production and personal savings and quarterly changes in the unemployment rate for the US over the period 1970 Q1 - 2019 Q2.

Below is the scatterplot matrix of all five variables: (`GGally` package must be installed already)

```{r}
us_change |>
  GGally::ggpairs(columns = 2:6)
```

The first column shows the relationships between the forecast variable (consumption) and each of the predictors. The scatterplots show positive relationships with income and industrial production, and negative relationships with savings and unemployment. The strength of these relationships are shown by the correlation coefficients across the first row. The remaining scatterplots and correlation coefficients show the relationships between the predictors.

# 7.1.3 Assumptions for the linear model

For forecasting purposes, we require the following assumptions:

* $\epsilon_t$ have mean zero and are uncorrelated.

* $\epsilon_t$ are uncorrelated with each $x_j,t$ for $i = 1,2, \dots, k$ variables.

It is useful to also have a normally distributed $\epsilon_t$ when producing prediction intervals or doing statistical tests.


# 7.2 Least squares estimation

The least squares principle provides a way of choosing the coefficients effectively by minimising the sum of the squared errors.

# 7.2.1 Fitted values

To get fitted values based on our example:

```{r}
fit_consMR <- us_change |>
  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings))

report(fit_consMR)
```

Based on the result, It can be seen that p-value of the F-statistic is < 2.2e-16, which is highly significant. 
Also, It reveals that all predictor variables are significantly associated to the change in consumption. 

The model can be written as 

Consumption $= 0.253 + 0.741 (Income) + 0.047 (Production) - 0.175 (Unemployment) - 0.053(Savings) + \epsilon$

Here, the income coefficient suggests that for every 1 unit increase in personal disposable income, holding all other predictors constant, we can expect an increase of 0.74% in personal consumption expenditure, on average.

Time plot of actual US consumption expenditure and predicted US consumption expenditure is shown below.

```{r}
augment(fit_consMR) |>
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Consumption, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = NULL,
    title = "Percent change in US consumption expenditure"
  ) +
  scale_colour_manual(values=c(Data="black",Fitted="#D55E00")) +
  guides(colour = guide_legend(title = NULL))
```
This is verified by the strong positive relationship shown by the scatterplot.

```{r}
augment(fit_consMR) |>
  ggplot(aes(x = Consumption, y = .fitted)) +
  geom_point() +
  labs(
    y = "Fitted (predicted values)",
    x = "Data (actual values)",
    title = "Percent change in US consumption expenditure"
  ) +
  geom_abline(intercept = 0, slope = 1)
```

# 7.2.2 Goodness-of-fit

The overall quality of the model can be assessed by examining the R-squared ($R^2$) and Residual Standard Error (RSE).  

Here, $R^2$ lies between 0 and 1. As shown also in `report(fit_consMR)`, the correlation between the actual consumption expenditure values versus the fitted values is $r=0.877$. Therefore, $R^2=0.768$. In this case, the model does an excellent job as it explains 76.8% of the variation in the consumption data compare that to the $R^2$ value of 0.15 obtained from the simple regression with the same data set in Section 7.1. Adding the three extra predictors has allowed a lot more of the variation in the consumption data to be explained.

# 7.2.3 Standard error of the regression

Another measure of how well the model has fitted the data is the standard deviation of the residuals, which is often known as the “residual standard error”. This is shown in the above output with the value 0.31. Note that the lower the RSE, the more accurate the model.


# 7.3 Evaluating a regression model

After selecting the regression variables and fitting a regression model, it is necessary to plot the residuals to check that the assumptions of the model have been satisfied such as ACF plot of residuals to find autocorrelation in the residuals and histogram of residuals for detecting violation of normality assumption.

# Example

To analyze the residuals from the fitted model `fit_consMR` from the previous example, `gg_tsresiduals()` function is used as this can obtain all the useful residual diagnostics mentioned above.


```{r}
fit_consMR |> gg_tsresiduals()
```

The time plot shows some changing variation over time, but is otherwise relatively unremarkable. This heteroscedasticity will potentially make the prediction interval coverage inaccurate.

The histogram shows that the residuals seem to be slightly skewed, which may also affect the coverage probability of the prediction intervals.

The autocorrelation plot shows a significant spike at lag 7.

Here, Ljung-Box test is significant at the 5% level ($H_0$ being the residuals are from a white noise series):

```{r}
augment(fit_consMR) |>
  features(.innov, ljung_box, lag = 10)
```


# 7.3.1 Residual plots against predictors

We would expect the residuals to be randomly scattered without showing any systematic patterns. Examining scatterplots of the residuals against each of the predictor variables is a quick and easy technique to verify this. If these scatterplots show a pattern, then the relationship may be nonlinear and the model will need to be modified accordingly.

# Example

`residuals()` allow us to extract residuals from a fable object, without calling `augment()`.

```{r}
us_change |>
  left_join(residuals(fit_consMR), by = "Quarter") |>
  pivot_longer(Income:Unemployment,
               names_to = "regressor", values_to = "x") |>
  ggplot(aes(x = x, y = .resid)) +
  geom_point() +
  facet_wrap(. ~ regressor, scales = "free_x") +
  labs(y = "Residuals", x = "")
```

The residuals from the multiple regression model for forecasting US consumption plotted against each predictor seem to be randomly scattered. Therefore we are satisfied with these in this case.

# 7.3.2 Residual plots against fitted values

A plot of the residuals against the fitted values should also show no pattern. If a pattern is observed, there may be **“heteroscedasticity”** in the errors which means that the variance of the residuals may not be constant. If this problem occurs, a transformation of the forecast variable such as a logarithm or square root may be required.

# Example

Continuing the previous example, the following plot shows the residuals plotted against the fitted values. 

```{r}
augment(fit_consMR) |>
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() + labs(x = "Fitted", y = "Residuals")
```

The random scatter suggests the errors are homoscedastic.

# 7.3.5 Outliers and influential observations

Observations that take extreme values compared to the majority of the data are called **outliers**. Observations that have a large influence on the estimated coefficients of a regression model are called **influential observations**. Usually, influential observations are also outliers that are extreme in the $x$ direction.

# 7.3.6 Spurious regression

Here we need to address the effect that non-stationary data can have on regression models.

For example, consider `guinea_rice` and `aus_airpassengers` plotted below. These appear to be related simply because they both trend upwards in the same manner. However, air passenger traffic in Australia has nothing to do with rice production in Guinea.

```{r}
guinea_rice |> 
  autoplot() + 
  ggtitle("Guinea rice production")

aus_airpassengers |>
  autoplot() + 
  ggtitle("Australia air passengers")
```


```{r}
guinea_rice |>
    left_join(aus_airpassengers) |>
    ggplot(aes(Production, Passengers)) + 
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) + 
    ggtitle("A well-fitted regression line")
```

# 7.4 Some useful predictors

There are several useful predictors that occur frequently when using regression for time series data.

# 7.4.1 Trend

It is common for time series data to be trending.A linear trend can be modelled by simply using $x_{1,t} = t$ as a predictor, $y_t = \beta_0 + \beta_1 t + \epsilon_t$ where $t=1, \dots, T$. A trend variable can be specified in the `TSLM()` function using the `trend()` special.

# 7.4.2 Dummy variables

If a categorical variable takes only two values (e.g., “yes” and “no”), then an equivalent numerical variable can be constructed taking value 1 if yes and 0 if no. This is called dummy variable(also known as indicator variable). If there are more than two categories, then the variable can be coded using several dummy variable (one fewer than the total number of categories).

# 7.4.3 Seasonal dummy variables

If the time series data shows strong seasonality, we tend to add seasonal dummy variables to include this seasonality in our model.

**Seasonal dummies**
For quarterly data: use 3 dummies
For monthly data: use 11 dummies
For daily data: use 6 dummies 

**Outliers**
If there is an outlier, you can use a dummy variable to remove its effect.

**Public holidays**
For daily data: If it is a public holiday, dummy=1, otherwise dummy=0.

An example of interpreting estimated dummy variable coefficients capturing the quarterly seasonality of Australian beer production follows.

# Example: Australian quarterly beer production

```{r}
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)
recent_production |>
  autoplot(Beer) +
  labs(y = "Megalitres",
       title = "Australian quarterly beer production")
```

Here, We want to forecast the value of future beer production. We can model this data using a regression model with a linear trend and quarterly dummy variables. The first quarter variable has been omitted, so the coefficients associated with the other quarters are measures of the difference between those quarters and the first quarter.

The `TSLM()` function will automatically handle this situation if you specify the special `season()`. Note that `trend()` and `season()` are not standard functions; they are “special” functions that work within the `TSLM()` model formulae.

```{r}
fit_beer <- recent_production |>
  model(TSLM(Beer ~ trend() + season()))

report(fit_beer)
```

You can see from the output that the trend variable is -0.34, that is, the beer production is decreasing by 0.34% megalitres per quarter. Here, we have three quarterly dummy variables (`season()year2`, `season()year3`, and `season()year4`). On average, the second quarter has production of 34.7 megalitres lower than the first quarter, the third quarter has production of 17.8 megalitres lower than the first quarter, and the fourth quarter has production of 72.8 megalitres higher than the first quarter.

The time plot of beer production and predicted beer production is shown below:
 
```{r}
augment(fit_beer) |>
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Beer, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  scale_colour_manual(
    values = c(Data = "black", Fitted = "#D55E00")
  ) +
  labs(y = "Megalitres",
       title = "Australian quarterly beer production") +
  guides(colour = guide_legend(title = "Series"))
```

As we look at the last peaks, it has been overestimated and we suggest to plot the fitted value against the actual value.

Here, the plot of Actual beer production plotted against predicted beer production:

```{r}
augment(fit_beer) |>
  ggplot(aes(x = Beer, y = .fitted,
             colour = factor(quarter(Quarter)))) +
  geom_point() +
  labs(y = "Fitted", x = "Actual values",
       title = "Australian quarterly beer production") +
  geom_abline(intercept = 0, slope = 1) +
  guides(colour = guide_legend(title = "Quarter"))
```

As we observe on the four quarter, the points are not lying along the line, which means those observations are overestimated at the end of the series and underestimated the start of the series.

Here's the plot of the residuals with ACF and the histogram:

```{r}
fit_beer |> gg_tsresiduals()
```
It can be seen that there is no problem with the residuals.

Here, the forecast obtained from that model:

```{r}
fit_beer |>
  forecast() |>
  autoplot(recent_production)
```

# 7.4.4 Intervention variables

When the effect lasts only for one period, we use a “spike” variable. This is a dummy variable that takes value one in the period of the intervention and zero elsewhere. A spike variable is equivalent to a dummy variable for handling an outlier.

Other interventions have an immediate and permanent effect. If an intervention causes a level shift (i.e., the value of the series changes suddenly and permanently from the time of intervention), then we use a “step” variable. A step variable takes value zero before the intervention and one from the time of intervention onward.

Another form of permanent effect is a change of slope. Here the intervention is handled using a piecewise linear trend; a trend that bends at the time of intervention and hence is nonlinear. 

# 7.4.5 Trading days

The number of trading days in a month can vary considerably and can have a substantial effect on sales data. To allow for this, the number of trading days in each month can be included as a predictor.

# 7.4.6 Distributed lags

It is often useful to include advertising expenditure as a predictor. However, since the effect of advertising can last beyond the actual campaign, we need to include lagged values of advertising expenditure. Thus, the following predictors may be used.

# 7.4.7 Easter

Easter differs from most holidays because it is not held on the same date each year, and its effect can last for several days. In this case, a dummy variable can be used with value one where the holiday falls in the particular time period and zero otherwise.

With monthly data, if Easter falls in March then the dummy variable takes value 1 in March, and if it falls in April the dummy variable takes value 1 in April. When Easter starts in March and finishes in April, the dummy variable is split proportionally between months.

# 7.4.8 Fourier series

An alternative to using seasonal dummy variables, especially for long seasonal periods, is to use Fourier terms.

Fourier terms are produced using the `fourier()` function. For example, the Australian beer data can be modelled like this.

```{r}
fourier_beer <- recent_production |>
  model(TSLM(Beer ~ trend() + fourier(K = 2)))
report(fourier_beer)
```

The results are identical to those obtained when using seasonal dummy variables.


# 7.5 Selecting predictors

When there are many possible predictors, we need some strategy for selecting the best predictors to use in a regression model. Here we use predictive accuracy. They can be shown using the `glance()` function, here applied to the model for `us_change`:

```{r}
glance(fit_consMR) |>
  select(adj_r_squared, CV, AIC, AICc, BIC)
```

We compare these values against the corresponding values from other models. For the CV, AICc, AIC and BIC measures, we want to find the model with the lowest value; for Adjusted $R^2$, we seek the model with the highest value.

# Example: US consumption

In `fit_consMR` of US consumption, four predictors are specified, so there are $2^4 = 16$ possible models.

Now we can check if all four predictors are actually useful, or whether we can drop one or more of them. 

The best model contains all four predictors according to AICc. The results from a backward selection using AIC follow suit:

# Best subset regression

Where possible, all potential regression models should be fitted (as was done in the example above) and the best model should be selected based on one of the measures discussed. This is known as “best subsets” regression or “all possible subsets” regression.


# Stepwise regression

If there are a large number of predictors, it is not possible to fit all possible models.

An approach that works quite well is backwards stepwise regression:

* Start with the model containing all potential predictors.

* Remove one predictor at a time. Keep the model if it improves the measure of predictive accuracy.

* Iterate until no further improvement.

It is important to realise that any stepwise approach is not guaranteed to lead to the best possible model, but it almost always leads to a good model.


# 7.6 Forecasting with regression

Here, we are interested in forecasting future values of $y$.

# 7.6.1 Ex-ante versus ex-post forecasts

**Ex-ante forecast** is a forecast based solely on information available at the time of the forecast, whereas **ex-post forecast** is a forecast that uses information beyond the time at which the forecast is made.

# 7.6.2 Example: Australian quarterly beer production

Normally, we cannot use actual future values of the predictor variables when producing ex-ante forecasts because their values will not be known in advance. However, the special predictors are all known in advance (a trend variable and 3 seasonal dummy variable), as they are based on calendar variables (e.g., seasonal dummy variables or public holiday indicators) or deterministic functions of time (e.g. time trend). In such cases, there is no difference between ex-ante and ex-post forecasts.

Here, the forecasts from the regression model for beer production. The dark shaded region shows 80% prediction intervals and the light shaded region shows 95% prediction intervals.

```{r}
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)
fit_beer <- recent_production |>
  model(TSLM(Beer ~ trend() + season()))
fc_beer <- forecast(fit_beer)
fc_beer |>
  autoplot(recent_production) +
  labs(
    title = "Forecasts of beer production using regression",
    y = "megalitres"
  )
```


# 7.6.3 Scenario based forecasting

In this setting, the forecaster assumes possible scenarios for the predictor variables that are of interest. For example, a US policy maker may be interested in comparing the predicted change in consumption when there is a constant growth of  
1% and 0.5% respectively for income and savings with no change in production and the employment rate, versus a respective decline of 1% and 0.5%, for each of the four quarters following the end of the sample.

We should note that prediction intervals for scenario based forecasts do not include the uncertainty associated with the future values of the predictor variables. They assume that the values of the predictors are known in advance, (i.e, ex-post forecasts).

The resulting forecasts are calculated and shown below:

```{r}
fit_consBest <- us_change |>
  model(
    lm = TSLM(Consumption ~ Income + Savings + Unemployment)
  )
future_scenarios <- scenarios(
  Increase = new_data(us_change, 4) |>
    mutate(Income=1, Savings=0.5, Unemployment=0),
  Decrease = new_data(us_change, 4) |>
    mutate(Income=-1, Savings=-0.5, Unemployment=0),
  names_to = "Scenario")

fc <- forecast(fit_consBest, new_data = future_scenarios)

us_change |>
  autoplot(Consumption) +
  autolayer(fc) +
  labs(title = "US consumption", y = "% change")
```

# 7.6.4 Building a predictive regression model
# 7.6.5 Prediction intervals

**Example:**

The estimated simple regression line in the US consumption example is $\hat{y}_t = 0.54 + 0.27  x_t$.

Assuming that for the next four quarters, personal income will increase by its historical mean value of $\bar{x} = 0.73%$, consumption is forecast to increase by $0.74%$ and the corresponding $80%$ and $95%$ prediction intervals are $[-0.02, 1.5]$ and $[-0.42, 1.9]$ respectively. If we assume an extreme increase of $12%$ in income, then the prediction intervals are considerably wider as shown below:

```{r}
fit_cons <- us_change |>
  model(TSLM(Consumption ~ Income))
new_cons <- scenarios(
  "Average increase" = new_data(us_change, 4) |>
    mutate(Income = mean(us_change$Income)),
  "Extreme increase" = new_data(us_change, 4) |>
    mutate(Income = 12),
  names_to = "Scenario"
)
fcast <- forecast(fit_cons, new_cons)

us_change |>
  autoplot(Consumption) +
  autolayer(fcast) +
  labs(title = "US consumption", y = "% change")
```


# 7.7 Nonlinear regression

In this section, we assume that we only have one predictor $x$. The simplest way of modelling a nonlinear relationship is to transform the forecast variable $y$ and/or the predictor variable $x$ before estimating a regression model. While this provides a non-linear functional form, the model is still linear in the parameters. The most commonly used transformation is the (natural) logarithm.

A **log-log** functional form is specified as
$logy = \beta_0 + \beta_1 logx + \epsilon$

The **log-linear** form is specified by only transforming the forecast variable and the **linear-log** form is obtained by transforming the predictor.

# 7.7.1 Forecasting with a nonlinear trend
# Example: Boston marathon winning times

We will fit some trend models to the Boston marathon winning times for men since the event started in 1897. First we extract the men’s data and convert the winning times to a numerical value. The course was lengthened (from 24.5 miles to 26.2 miles) in 1924, which led to a jump in the winning times, so we only consider data from that date onwards.

The plot below shows the fitted lines and forecasts from linear, exponential and piecewise linear trends. The best forecasts appear to come from the piecewise linear trend.

```{r}
boston_men <- boston_marathon |>
  filter(Year >= 1924) |>
  filter(Event == "Men's open division") |>
  mutate(Minutes = as.numeric(Time)/60)

fit_trends <- boston_men |>
  model(
    linear = TSLM(Minutes ~ trend()),
    exponential = TSLM(log(Minutes) ~ trend()),
    piecewise = TSLM(Minutes ~ trend(knots = c(1950, 1980)))
  )
fc_trends <- fit_trends |> forecast(h = 10)

boston_men |>
  autoplot(Minutes) +
  geom_line(data = fitted(fit_trends),
            aes(y = .fitted, colour = .model)) +
  autolayer(fc_trends, alpha = 0.5, level = 95) +
  labs(y = "Minutes",
       title = "Boston marathon winning times")
```


# 7.8 Correlation, causation and forecasting
# 7.8.1 Correlation is not causation

When $x$ is useful for predicting $y$, it is not necessarily causing $y$ (e.g., predict number of swimmers $y$ using number of ice-crreams sold $x$). Correlations are useful for forecasting, even when there is no causality. Better models usually involve causal relationship (e.g., temperature $x$ and people $z$ to predict swimmers $y$).

# 7.8.2 Forecasting with correlated predictors

When two or more predictors are highly correlated it is always challenging to accurately separate their individual effects.

Suppose we are forecasting monthly sales of a company for 2012, using data from 2000–2011. In January 2008, a new competitor came into the market and started taking some market share. At the same time, the economy began to decline. In your forecasting model, you include both competitor activity (measured using advertising time on a local television station) and the health of the economy (measured using GDP). It will not be possible to separate the effects of these two predictors because they are highly correlated.

# 7.8.3 Multicollinearity and forecasting

In regression analysis, multicollinearity occurs when two predictors are highly correlated (i.e., the correlation between them is close to $\pm 1$). A linear combination of one subset of predictors is highly correlated with a linear combination of another subset of predictors.

When multicollinearity is present, the uncertainty associated with individual regression coefficients will be large. This is because they are difficult to estimate. Consequently, statistical tests (e.g., t-tests) on regression coefficients are unreliable. (In forecasting we are rarely interested in such tests.) Also, it will not be possible to make accurate statements about the contribution of each separate predictor to the forecast.
 